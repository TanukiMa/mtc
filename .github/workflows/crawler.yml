name: åŽšåŠ´çœã‚µã‚¤ãƒˆå°‚é–€ç”¨èªžè§£æž

on:
  schedule:
    # æ¯Žæ—¥åˆå‰2æ™‚ï¼ˆJST 11æ™‚ï¼‰ã«å®Ÿè¡Œ
    - cron: '0 2 * * *'
  workflow_dispatch:  # æ‰‹å‹•å®Ÿè¡Œã‚‚å¯èƒ½
    inputs:
      max_workers:
        description: 'ä¸¦åˆ—å‡¦ç†æ•°'
        required: false
        default: '3'
        type: string

env:
  PYTHON_VERSION: '3.11'
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}

jobs:
  analyze-mhlw-terms:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2æ™‚é–“ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          openjdk-11-jre-headless \
          build-essential \
          cmake \
          pkg-config
        
        # Javaç’°å¢ƒå¤‰æ•°è¨­å®š
        echo "JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> $GITHUB_ENV
        echo "PATH=$PATH:/usr/lib/jvm/java-11-openjdk-amd64/bin" >> $GITHUB_ENV
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install \
          requests==2.31.0 \
          beautifulsoup4==4.12.2 \
          supabase==2.0.2 \
          sudachipy==0.6.7 \
          huggingface_hub==0.19.4 \
          python-docx==0.8.11 \
          python-pptx==0.6.22 \
          PyPDF2==3.0.1 \
          lxml==4.9.3
    
    - name: Download Sudachi dictionary
      run: |
        mkdir -p ~/.sudachi
        # Sudachiè¾žæ›¸ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå…¬å¼ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ï¼‰
        wget -O ~/.sudachi/system.dic \
          https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/sudachi-dictionary-20230110/system_core.dic
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        cat > ~/.sudachi/sudachi.json << 'EOF'
        {
          "systemDict": "~/.sudachi/system.dic",
          "characterDefinitionFile": "char.def",
          "inputTextPlugin": [],
          "oovProviderPlugin": [],
          "pathRewritePlugin": [],
          "connectPlugin": []
        }
        EOF
    
    - name: Install Hugging Face CLI and llama.cpp (from Debian sid)
      run: |
        pip install huggingface_hub
        
        # Debian sid (unstable) ãƒªãƒã‚¸ãƒˆãƒªã‚’è¿½åŠ ã—ã¦llama.cppã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        echo "â„¹ï¸  Adding Debian sid repository for llama.cpp package"
        
        # sidãƒªãƒã‚¸ãƒˆãƒªè¿½åŠ ï¼ˆæœ€å°é™ã®å½±éŸ¿ã«é™å®šï¼‰
        echo "deb http://deb.debian.org/debian sid main" | sudo tee /etc/apt/sources.list.d/sid.list
        
        # å„ªå…ˆåº¦è¨­å®šï¼ˆsidã‹ã‚‰ã¯å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã¿ï¼‰
        sudo tee /etc/apt/preferences.d/llama-cpp-pin << 'EOF'
        Package: llama.cpp llama.cpp-tools
        Pin: release a=sid
        Pin-Priority: 500
        
        Package: *
        Pin: release a=sid
        Pin-Priority: 100
        EOF
        
        # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒªã‚¹ãƒˆæ›´æ–°ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        sudo apt update
        sudo apt install -y llama.cpp
        
        # å‹•ä½œç¢ºèª
        llama-cli --help | head -5
        echo "LLAMA_CLI_PATH=llama-cli" >> $GITHUB_ENV
    
    - name: Download LLM model with huggingface-cli
      run: |
        mkdir -p models
        
        # æ—¥æœ¬èªžå¯¾å¿œã®è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        huggingface-cli download \
          mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf \
          ELYZA-japanese-Llama-2-7b-fast-instruct-q4_0.gguf \
          --local-dir models \
          --local-dir-use-symlinks False
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’çµ±ä¸€
        mv models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_0.gguf models/ggml-model-Q4_K_M.gguf
        
        echo "LLAMA_MODEL_PATH=$(pwd)/models/ggml-model-Q4_K_M.gguf" >> $GITHUB_ENV
        echo "LLAMA_CLI_PATH=/usr/local/bin/llama-cli" >> $GITHUB_ENV
    
    - name: Create cache directories
      run: |
        mkdir -p ~/.cache/sudachi
        mkdir -p /tmp
    
    - name: Verify environment
      run: |
        python --version
        java -version
        llama-cli --help | head -5
        echo "SUPABASE_URL: ${SUPABASE_URL:0:20}..."
        echo "Model path: $LLAMA_MODEL_PATH"
        echo "CLI path: $LLAMA_CLI_PATH"
        ls -la models/
        file models/ggml-model-Q4_K_M.gguf
    
    - name: Initialize Supabase dictionary
      run: |
        python3 << 'EOF'
        import os
        from supabase import create_client
        
        # åŸºæœ¬è¾žæ›¸ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ï¼ˆåˆå›žã®ã¿ï¼‰
        client = create_client(os.environ['SUPABASE_URL'], os.environ['SUPABASE_KEY'])
        
        # æ—¢å­˜è¾žæ›¸å˜èªžã®ä¾‹ï¼ˆå®Ÿéš›ã«ã¯ã‚‚ã£ã¨å¤§é‡ã«æŠ•å…¥ï¼‰
        basic_words = [
            {'word': 'åŒ»ç™‚', 'reading': 'ã‚¤ãƒªãƒ§ã‚¦', 'part_of_speech': 'åè©ž', 'source': 'basic'},
            {'word': 'åŽšç”ŸåŠ´åƒçœ', 'reading': 'ã‚³ã‚¦ã‚»ã‚¤ãƒ­ã‚¦ãƒ‰ã‚¦ã‚·ãƒ§ã‚¦', 'part_of_speech': 'åè©ž', 'source': 'basic'},
            {'word': 'å¥åº·', 'reading': 'ã‚±ãƒ³ã‚³ã‚¦', 'part_of_speech': 'åè©ž', 'source': 'basic'},
        ]
        
        try:
            # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
            existing = client.table('dictionary_words').select('word').limit(1).execute()
            if not existing.data:
                client.table('dictionary_words').insert(basic_words).execute()
                print("åŸºæœ¬è¾žæ›¸ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
            else:
                print("è¾žæ›¸ã¯æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã§ã™")
        except Exception as e:
            print(f"è¾žæ›¸åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        EOF
    
    - name: Run crawler analysis
      env:
        MAX_WORKERS: ${{ github.event.inputs.max_workers || '3' }}
      run: |
        python3 << 'EOF'
        import sys
        import os
        sys.path.append('.')
        
        # ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼å®Ÿè¡Œ
        try:
            from main_crawler import MhlwCrawler
            crawler = MhlwCrawler()
            max_workers = int(os.environ.get('MAX_WORKERS', '3'))
            crawler.run(max_workers=max_workers)
            print("âœ… ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼å®Ÿè¡Œå®Œäº†")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
        EOF
    
    - name: Generate summary report
      if: always()
      run: |
        python3 << 'EOF'
        import os
        from supabase import create_client
        from datetime import datetime, timedelta
        
        client = create_client(os.environ['SUPABASE_URL'], os.environ['SUPABASE_KEY'])
        
        # ä»Šæ—¥ã®çµæžœã‚µãƒžãƒªãƒ¼
        today = datetime.now().date()
        
        # æ–°èªžå€™è£œæ•°
        new_words = client.table('new_word_candidates')\
            .select('*')\
            .gte('created_at', today.isoformat())\
            .execute()
        
        # å‡¦ç†URLæ•°
        processed = client.table('processed_urls')\
            .select('*')\
            .gte('created_at', today.isoformat())\
            .execute()
        
        print(f"""
        ðŸ“Š å®Ÿè¡Œçµæžœã‚µãƒžãƒªãƒ¼
        ==================
        æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        å‡¦ç†æ¸ˆURL: {len(processed.data)}ä»¶
        æ–°èªžå€™è£œ: {len(new_words.data)}ä»¶
        
        ðŸ” æ–°èªžå€™è£œãƒˆãƒƒãƒ—5:
        """)
        
        for i, word in enumerate(new_words.data[:5], 1):
            print(f"{i}. {word['word']} ({word['reading']}) - ä¿¡é ¼åº¦: {word['confidence_score']:.2f}")
        
        print("\nâœ… è§£æžå®Œäº†")
        EOF
    
    - name: Cleanup temporary files
      if: always()
      run: |
        rm -rf /tmp/*.pdf /tmp/*.docx /tmp/*.pptx
        rm -rf models/  # å¤§ãã„ã®ã§å‰Šé™¤ï¼ˆæ¬¡å›žå†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
    
    - name: Upload logs (on failure)
      if: failure()
      uses: actions/upload-artifact@v3
      with:
        name: crawler-logs
        path: |
          *.log
          /tmp/crawler_*.txt
        retention-days: 7
