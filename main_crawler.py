#!/usr/bin/env python3
"""
åšåŠ´çœã‚µã‚¤ãƒˆå°‚é–€ç”¨èªæ–°èªç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ ï¼ˆè¾æ›¸ãƒ™ãƒ¼ã‚¹åˆ¤å®šï¼‰
GitHub Actions + Supabase + SudachiDict-full
"""

import os
import re
import hashlib
import logging
from pathlib import Path
from typing import List, Set, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse
import uuid
import time

import requests
from bs4 import BeautifulSoup
import supabase
from supabase import create_client, Client
from sudachipy import tokenizer, dictionary
from docx import Document
from pptx import Presentation
import PyPDF2

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SupabaseClient:
    """Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    def __init__(self):
        self.client: Client = create_client(
            os.environ['SUPABASE_URL'],
            os.environ['SUPABASE_KEY']
        )
    
    def is_url_processed(self, url: str, content_hash: str) -> bool:
        """URLãŒæ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            result = self.client.table('processed_urls')\
                .select('*')\
                .eq('url', url)\
                .eq('file_hash', content_hash)\
                .execute()
            return len(result.data) > 0
        except Exception as e:
            logger.warning(f"URLå‡¦ç†æ¸ˆã¿ç¢ºèªã§ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def save_processed_url(self, url: str, content_type: str, content_hash: str) -> str:
        """å‡¦ç†æ¸ˆã¿URLä¿å­˜"""
        try:
            result = self.client.table('processed_urls').insert({
                'url': url,
                'content_type': content_type,
                'file_hash': content_hash,
                'status': 'completed'
            }).execute()
            return result.data[0]['id'] if result.data else None
        except Exception as e:
            logger.error(f"URLä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def save_extracted_words(self, words: List[Dict], url_id: str):
        """æŠ½å‡ºå˜èªä¿å­˜"""
        if not words or not url_id:
            return
            
        try:
            for word_data in words:
                word_data['url_id'] = url_id
            self.client.table('extracted_words').insert(words).execute()
        except Exception as e:
            logger.error(f"å˜èªä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_dictionary_words(self) -> Set[str]:
        """æ—¢å­˜è¾æ›¸å˜èªå–å¾—"""
        try:
            result = self.client.table('dictionary_words').select('word').execute()
            return {row['word'] for row in result.data}
        except Exception as e:
            logger.warning(f"è¾æ›¸å˜èªå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return set()
    
    def save_new_word_candidate(self, word_data: Dict):
        """æ–°èªå€™è£œä¿å­˜"""
        try:
            self.client.table('new_word_candidates').insert(word_data).execute()
        except Exception as e:
            logger.error(f"æ–°èªå€™è£œä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

class DocumentProcessor:
    """æ–‡æ›¸å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def extract_text_from_html(content: str) -> str:
        """HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        try:
            soup = BeautifulSoup(content, 'html.parser')
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ»ã‚¹ã‚¿ã‚¤ãƒ«å‰Šé™¤
            for script in soup(["script", "style"]):
                script.decompose()
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            text = soup.get_text()
            # æ”¹è¡Œãƒ»ç©ºç™½ã®æ­£è¦åŒ–
            text = re.sub(r'\s+', ' ', text).strip()
            return text
        except Exception as e:
            logger.error(f"HTMLå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_pdf(file_path: str) -> str:
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        text = ""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
        except Exception as e:
            logger.error(f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return text
    
    @staticmethod
    def extract_text_from_docx(file_path: str) -> str:
        """DOCXã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        try:
            doc = Document(file_path)
            return '\n'.join([paragraph.text for paragraph in doc.paragraphs if paragraph.text])
        except Exception as e:
            logger.error(f"DOCXå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_pptx(file_path: str) -> str:
        """PPTXã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        try:
            prs = Presentation(file_path)
            text = ""
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text:
                        text += shape.text + "\n"
            return text
        except Exception as e:
            logger.error(f"PPTXå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

class SudachiAnalyzer:
    """Sudachiå½¢æ…‹ç´ è§£æï¼ˆSudachiDict-fullä½¿ç”¨ï¼‰"""
    
    def __init__(self):
        # SudachiPyã§Fullè¾æ›¸ã‚’ä½¿ç”¨ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«çµŒç”±ï¼‰
        from sudachipy import tokenizer, dictionary
        
        try:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            self.tokenizer_obj = dictionary.Dictionary().create()
            self.mode = tokenizer.Tokenizer.SplitMode.A
            logger.info("âœ… SudachiDict-full ã‚’ä½¿ç”¨ã—ã¦åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"Sudachiè¾æ›¸ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            raise
    
    def analyze(self, text: str) -> List[Dict]:
        """ãƒ†ã‚­ã‚¹ãƒˆè§£æ"""
        if not text:
            return []
            
        words = []
        try:
            tokens = self.tokenizer_obj.tokenize(text, self.mode)
            
            for token in tokens:
                # å°‚é–€ç”¨èªã‚‰ã—ã„ã‚‚ã®ï¼ˆåè©ã€è¤‡åˆèªãªã©ï¼‰ã‚’æŠ½å‡º
                pos = token.part_of_speech()[0]
                surface = token.surface()
                
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶
                if (pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©'] and 
                    len(surface) >= 2 and 
                    not surface.isdigit() and
                    surface not in ['ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ãªã©']):
                    
                    words.append({
                        'word': surface,
                        'reading': token.reading_form() or surface,
                        'part_of_speech': pos,
                    })
        except Exception as e:
            logger.error(f"å½¢æ…‹ç´ è§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return words
    
    def is_known_word(self, word: str) -> bool:
        """SudachiDict-fullã«åè¼‰ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            # è¾æ›¸ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹èªå½™ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            tokens = self.tokenizer_obj.tokenize(word, self.mode)
            
            # 1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ãªã‚Šã€ã‹ã¤æœªçŸ¥èªã§ãªã„å ´åˆã¯æ—¢çŸ¥èª
            if len(tokens) == 1:
                token = tokens[0]
                # æœªçŸ¥èªã®å ´åˆã€å“è©ã«ã€Œè£œåŠ©è¨˜å·ã€ç­‰ãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒå¤šã„
                pos_features = token.part_of_speech()
                if ('æœªçŸ¥èª' not in str(pos_features) and 
                    'è£œåŠ©è¨˜å·' not in str(pos_features) and
                    token.surface() == word):
                    return True
            
            return False
        except Exception as e:
            logger.warning(f"è¾æ›¸æ¤œç´¢ã‚¨ãƒ©ãƒ¼ '{word}': {e}")
            return False

class NewWordDetector:
    """è¾æ›¸ãƒ™ãƒ¼ã‚¹æ–°èªæ¤œå‡º"""
    
    def __init__(self, analyzer: SudachiAnalyzer):
        self.analyzer = analyzer
        
        # é™¤å¤–ã™ã‚‹ä¸€èˆ¬çš„ãªèªå½™ï¼ˆåšåŠ´çœæ–‡æ›¸ã§ã‚ˆãå‡ºç¾ã™ã‚‹åŸºæœ¬èªå½™ï¼‰
        self.common_words = {
            # ä¸€èˆ¬çš„ãªè¡Œæ”¿ãƒ»åŒ»ç™‚ç”¨èª
            'æ”¿ç­–', 'åˆ¶åº¦', 'å¯¾ç­–', 'æ–½ç­–', 'äº‹æ¥­', 'å–ã‚Šçµ„ã¿', 'æ¨é€²', 'æ”¯æ´',
            'å›½æ°‘', 'ç¤¾ä¼š', 'åœ°åŸŸ', 'å…¨å›½', 'éƒ½é“åºœçœŒ', 'å¸‚ç”ºæ‘',
            'åšç”Ÿ', 'åŠ´åƒ', 'å¥åº·', 'åŒ»ç™‚', 'ä»‹è­·', 'ç¦ç¥‰', 'å¹´é‡‘', 'ä¿é™º',
            'ç½å®³', 'è·å ´', 'åŠ´åƒè€…', 'äº‹æ¥­è€…', 'é–¢ä¿‚è€…',
            # åŸºæœ¬èªå½™
            'ä»Šå›', 'ä»Šå¾Œ', 'ç¾åœ¨', 'éå»', 'å°†æ¥', 'çŠ¶æ³', 'èª²é¡Œ', 'å•é¡Œ',
            'æ–¹æ³•', 'æ‰‹æ³•', 'ä»•çµ„ã¿', 'ä½“åˆ¶', 'ç’°å¢ƒ', 'æ¡ä»¶', 'åŸºæº–',
            'åŠ¹æœ', 'å½±éŸ¿', 'çµæœ', 'æˆæœ', 'å®Ÿç¸¾', 'è©•ä¾¡'
        }
        
        logger.info("âœ… è¾æ›¸ãƒ™ãƒ¼ã‚¹æ–°èªæ¤œå‡ºå™¨ã‚’åˆæœŸåŒ–å®Œäº†")
    
    def is_new_word(self, word: str, part_of_speech: str) -> tuple[bool, float, str]:
        """è¾æ›¸ãƒ™ãƒ¼ã‚¹ã§æ–°èªã‹ã©ã†ã‹åˆ¤å®š"""
        
        # åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if (len(word) < 3 or  # 3æ–‡å­—æœªæº€ã¯é™¤å¤–
            word in self.common_words or  # ä¸€èˆ¬èªã¯é™¤å¤–
            word.isdigit() or  # æ•°å­—ã®ã¿ã¯é™¤å¤–
            not re.match(r'^[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾ a-zA-Z]+, word)):  # æ–‡å­—ç¨®ãƒã‚§ãƒƒã‚¯
            return False, 0.1, "åŸºæœ¬ãƒ•ã‚£ãƒ«ã‚¿ã§é™¤å¤–"
        
        # åè©ã«é™å®šï¼ˆæ–°èªå€™è£œã¨ã—ã¦æœ€ã‚‚æœ‰åŠ›ï¼‰
        if part_of_speech != 'åè©':
            return False, 0.2, "åè©ä»¥å¤–"
        
        # SudachiDict-fullã«åè¼‰ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        is_known = self.analyzer.is_known_word(word)
        
        if is_known:
            return False, 0.3, "SudachiDict-fullã«åè¼‰æ¸ˆã¿"
        else:
            # æ–°èªå€™è£œã¨ã—ã¦åˆ¤å®š
            confidence = 0.8  # è¾æ›¸ã«ãªã„å ´åˆã¯é«˜ã„ä¿¡é ¼åº¦
            
            # å°‚é–€ç”¨èªã‚‰ã—ã•ã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´
            if len(word) >= 5:  # 5æ–‡å­—ä»¥ä¸Šã¯å°‚é–€ç”¨èªã®å¯èƒ½æ€§é«˜
                confidence = 0.9
            elif any(char in word for char in ['DX', 'AI', 'IoT', 'ICT']):  # è‹±ç•¥èªå«ã‚€
                confidence = 0.9
            elif word.endswith(('ã‚·ã‚¹ãƒ†ãƒ ', 'äº‹æ¥­', 'åˆ¶åº¦', 'æ”¿ç­–')):  # å°‚é–€ç”¨èªãƒ‘ã‚¿ãƒ¼ãƒ³
                confidence = 0.7
            
            return True, confidence, f"SudachiDict-fullæœªåè¼‰ï¼ˆ{len(word)}æ–‡å­—ã®åè©ï¼‰"

class MhlwCrawler:
    """ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.db = SupabaseClient()
        self.processor = DocumentProcessor()
        self.analyzer = SudachiAnalyzer()
        self.detector = NewWordDetector(self.analyzer)  # LLMã§ã¯ãªãè¾æ›¸ãƒ™ãƒ¼ã‚¹
        
        self.base_url = "https://www.mhlw.go.jp"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; MHLW Terminology Research Bot; +https://github.com/)'
        })
        
        logger.info("ğŸš€ MhlwCrawleråˆæœŸåŒ–å®Œäº†ï¼ˆè¾æ›¸ãƒ™ãƒ¼ã‚¹æ–°èªæ¤œå‡ºï¼‰")
    
    def get_urls_to_crawl(self) -> List[str]:
        """ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å¯¾è±¡URLå–å¾—"""
        urls = []
        
        # åšåŠ´çœã®ä¸»è¦ãƒšãƒ¼ã‚¸ã‹ã‚‰é–‹å§‹
        start_urls = [
            f"{self.base_url}/stf/seisakunitsuite/bunya/kenkou_iryou/",
            f"{self.base_url}/stf/seisakunitsuite/bunya/koyou_roudou/",
            f"{self.base_url}/toukei/",
            f"{self.base_url}/shingi/",
        ]
        
        logger.info(f"ğŸ“¡ {len(start_urls)}å€‹ã®ã‚¹ã‚¿ãƒ¼ãƒˆURLã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹")
        
        for start_url in start_urls:
            try:
                logger.info(f"ğŸ” {start_url} ã‚’è§£æä¸­...")
                response = self.session.get(start_url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # PDF, DOCX, PPTXãƒªãƒ³ã‚¯åé›†
                file_links = 0
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if any(ext in href.lower() for ext in ['.pdf', '.docx', '.pptx']):
                        full_url = urljoin(start_url, href)
                        if full_url not in urls:
                            urls.append(full_url)
                            file_links += 1
                
                # HTMLãƒšãƒ¼ã‚¸ã‚‚è¿½åŠ ï¼ˆåŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ï¼‰
                html_links = 0
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if (href.startswith('/') or 'mhlw.go.jp' in href) and not any(ext in href.lower() for ext in ['.pdf', '.docx', '.pptx']):
                        full_url = urljoin(start_url, href)
                        if full_url not in urls and len(urls) < 100:  # ä¸Šé™è¨­å®š
                            urls.append(full_url)
                            html_links += 1
                
                logger.info(f"âœ… {start_url}: ãƒ•ã‚¡ã‚¤ãƒ«{file_links}ä»¶ã€HTML{html_links}ä»¶ã‚’ç™ºè¦‹")
                time.sleep(1)  # é–“éš”ã‚’ç©ºã‘ã‚‹
                
            except Exception as e:
                logger.error(f"âŒ URLåé›†ã‚¨ãƒ©ãƒ¼ {start_url}: {e}")
        
        # é‡è¤‡é™¤å»
        unique_urls = list(set(urls))
        logger.info(f"ğŸ¯ åˆè¨ˆ {len(unique_urls)} å€‹ã®URLã‚’åé›†å®Œäº†")
        
        return unique_urls[:50]  # æœ€åˆã®50å€‹ã«åˆ¶é™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    
    def process_url(self, url: str) -> Optional[Dict]:
        """å˜ä¸€URLå‡¦ç†"""
        try:
            logger.info(f"ğŸ”„ å‡¦ç†é–‹å§‹: {url}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            content_hash = hashlib.md5(response.content).hexdigest()
            
            # æ—¢å‡¦ç†ãƒã‚§ãƒƒã‚¯
            if self.db.is_url_processed(url, content_hash):
                logger.info(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å‡¦ç†ï¼‰: {url}")
                return None
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¤å®š
            content_type = self._get_content_type(url, response.headers.get('content-type', ''))
            
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            text = self._extract_text(response.content, content_type, url)
            if not text or len(text) < 50:
                logger.warning(f"âš ï¸  ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—ã¾ãŸã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¸è¶³: {url}")
                return None
            
            logger.info(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {len(text)}æ–‡å­—")
            
            # å½¢æ…‹ç´ è§£æ
            words = self.analyzer.analyze(text)
            logger.info(f"ğŸ”¤ å½¢æ…‹ç´ è§£æå®Œäº†: {len(words)}èªã‚’æŠ½å‡º")
            
            # æ—¢å­˜è¾æ›¸ã¨ç…§åˆ + æ–°èªæ¤œå‡º
            dictionary_words = self.db.get_dictionary_words()
            new_candidates = []
            
            # èªå½™ã®é »åº¦ã‚«ã‚¦ãƒ³ãƒˆï¼ˆåŒã˜æ–‡æ›¸å†…ã§ã®å‡ºç¾é »åº¦ï¼‰
            word_freq = {}
            for word_data in words:
                word = word_data['word']
                word_freq[word] = word_freq.get(word, 0) + 1
            
            # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªèªå½™ã®ã¿ã‚’æ–°èªå€™è£œã¨ã—ã¦æ¤œè¨
            unique_words = {}
            for word_data in words:
                word = word_data['word']
                if word not in unique_words:
                    unique_words[word] = word_data
            
            logger.info(f"ğŸ” æ–°èªæ¤œå‡ºé–‹å§‹: {len(unique_words)}èªã‚’ãƒã‚§ãƒƒã‚¯")
            
            for word, word_data in unique_words.items():
                # åŸºæœ¬è¾æ›¸ã«ãªã„èªå½™ã‚’ãƒã‚§ãƒƒã‚¯
                if word not in dictionary_words:
                    # è¾æ›¸ãƒ™ãƒ¼ã‚¹æ–°èªåˆ¤å®š
                    is_new, confidence, reasoning = self.detector.is_new_word(
                        word, word_data['part_of_speech']
                    )
                    
                    if is_new and confidence > 0.6:
                        frequency = word_freq.get(word, 1)
                        new_candidates.append({
                            'word': word,
                            'reading': word_data['reading'],
                            'part_of_speech': word_data['part_of_speech'],
                            'confidence_score': confidence,
                            'llm_reasoning': reasoning,  # åˆ¤å®šç†ç”±
                            'source_urls': [url],
                            'frequency_count': frequency
                        })
                        logger.info(f"âœ¨ æ–°èªå€™è£œç™ºè¦‹: '{word}' (ä¿¡é ¼åº¦: {confidence:.3f}, é »åº¦: {frequency})")
            
            # DBä¿å­˜
            url_id = self.db.save_processed_url(url, content_type, content_hash)
            if url_id and words:
                # èªæ•°åˆ¶é™
                words_to_save = words[:100] if len(words) > 100 else words
                self.db.save_extracted_words(words_to_save, url_id)
            
            for candidate in new_candidates:
                self.db.save_new_word_candidate(candidate)
            
            logger.info(f"âœ… å®Œäº†: {url} - æ–°èªå€™è£œ: {len(new_candidates)}ä»¶")
            return {
                'url': url,
                'words_count': len(words),
                'new_words_count': len(new_candidates)
            }
            
        except Exception as e:
            logger.error(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼ {url}: {e}")
            return None
    
    def _get_content_type(self, url: str, content_type_header: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—åˆ¤å®š"""
        url_lower = url.lower()
        if '.pdf' in url_lower:
            return 'pdf'
        elif '.docx' in url_lower:
            return 'docx'
        elif '.pptx' in url_lower:
            return 'pptx'
        else:
            return 'html'
    
    def _extract_text(self, content: bytes, content_type: str, url: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        if content_type == 'html':
            return self.processor.extract_text_from_html(content.decode('utf-8', errors='ignore'))
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã—ã¦å‡¦ç†
            temp_path = f"/tmp/{uuid.uuid4()}.{content_type}"
            try:
                with open(temp_path, 'wb') as f:
                    f.write(content)
                
                if content_type == 'pdf':
                    return self.processor.extract_text_from_pdf(temp_path)
                elif content_type == 'docx':
                    return self.processor.extract_text_from_docx(temp_path)
                elif content_type == 'pptx':
                    return self.processor.extract_text_from_pptx(temp_path)
            finally:
                Path(temp_path).unlink(missing_ok=True)
        
        return ""
    
    def run(self, max_workers: int = 3):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        logger.info("ğŸš€ åšåŠ´çœã‚µã‚¤ãƒˆè§£æé–‹å§‹ï¼ˆè¾æ›¸ãƒ™ãƒ¼ã‚¹æ–°èªæ¤œå‡ºï¼‰")
        start_time = time.time()
        
        # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å¯¾è±¡URLå–å¾—
        urls = self.get_urls_to_crawl()
        if not urls:
            logger.error("âŒ ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å¯¾è±¡URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        logger.info(f"ğŸ¯ å¯¾è±¡URLæ•°: {len(urls)}")
        
        total_processed = 0
        total_new_words = 0
        
        # ä¸¦åˆ—å‡¦ç†
        logger.info(f"ğŸ‘¥ ä¸¦åˆ—å‡¦ç†é–‹å§‹ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}ï¼‰")
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self.process_url, url): url for url in urls}
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    if result:
                        total_processed += 1
                        total_new_words += result['new_words_count']
                        logger.info(f"ğŸ“Š é€²æ—: {total_processed}/{len(urls)} å®Œäº†")
                except Exception as e:
                    logger.error(f"âŒ {url} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        
        elapsed_time = time.time() - start_time
        logger.info(f"ğŸ‰ å‡¦ç†å®Œäº†: {total_processed}URLå‡¦ç†, {total_new_words}æ–°èªå€™è£œç™ºè¦‹, {elapsed_time:.1f}ç§’")
        logger.info("ğŸ’¡ æ–°èªåˆ¤å®šåŸºæº–: SudachiDict-fullï¼ˆ170ä¸‡èªï¼‰æœªåè¼‰ã®åè©")

if __name__ == "__main__":
    # ç›´æ¥å®Ÿè¡Œç”¨
    import argparse
    
    parser = argparse.ArgumentParser(description='åšåŠ´çœã‚µã‚¤ãƒˆå°‚é–€ç”¨èªè§£æï¼ˆè¾æ›¸ãƒ™ãƒ¼ã‚¹ï¼‰')
    parser.add_argument('--workers', type=int, default=3, help='ä¸¦åˆ—å‡¦ç†æ•°')
    args = parser.parse_args()
    
    crawler = MhlwCrawler()
    crawler.run(max_workers=args.workers)
