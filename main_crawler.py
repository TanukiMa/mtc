#!/usr/bin/env python3
"""
åšåŠ´çœã‚µã‚¤ãƒˆå°‚é–€ç”¨èªæ–°èªç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ 
GitHub Actions + Supabase + Sudachi + llama.cpp
"""

import os
import re
import hashlib
import logging
import asyncio
import subprocess
import json
import tempfile
from pathlib import Path
from typing import List, Set, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse
import uuid
import time

import requests
from bs4 import BeautifulSoup
import supabase
from supabase import create_client, Client
from sudachipy import tokenizer, dictionary
from docx import Document
from pptx import Presentation
import PyPDF2

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SupabaseClient:
    """Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    def __init__(self):
        self.client: Client = create_client(
            os.environ['SUPABASE_URL'],
            os.environ['SUPABASE_KEY']
        )
    
    def is_url_processed(self, url: str, content_hash: str) -> bool:
        """URLãŒæ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            result = self.client.table('processed_urls')\
                .select('*')\
                .eq('url', url)\
                .eq('file_hash', content_hash)\
                .execute()
            return len(result.data) > 0
        except Exception as e:
            logger.warning(f"URLå‡¦ç†æ¸ˆã¿ç¢ºèªã§ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def save_processed_url(self, url: str, content_type: str, content_hash: str) -> str:
        """å‡¦ç†æ¸ˆã¿URLä¿å­˜"""
        try:
            result = self.client.table('processed_urls').insert({
                'url': url,
                'content_type': content_type,
                'file_hash': content_hash,
                'status': 'completed'
            }).execute()
            return result.data[0]['id'] if result.data else None
        except Exception as e:
            logger.error(f"URLä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def save_extracted_words(self, words: List[Dict], url_id: str):
        """æŠ½å‡ºå˜èªä¿å­˜"""
        if not words or not url_id:
            return
            
        try:
            for word_data in words:
                word_data['url_id'] = url_id
            self.client.table('extracted_words').insert(words).execute()
        except Exception as e:
            logger.error(f"å˜èªä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_dictionary_words(self) -> Set[str]:
        """æ—¢å­˜è¾æ›¸å˜èªå–å¾—"""
        try:
            result = self.client.table('dictionary_words').select('word').execute()
            return {row['word'] for row in result.data}
        except Exception as e:
            logger.warning(f"è¾æ›¸å˜èªå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return set()
    
    def save_new_word_candidate(self, word_data: Dict):
        """æ–°èªå€™è£œä¿å­˜"""
        try:
            self.client.table('new_word_candidates').insert(word_data).execute()
        except Exception as e:
            logger.error(f"æ–°èªå€™è£œä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

class DocumentProcessor:
    """æ–‡æ›¸å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def extract_text_from_html(content: str) -> str:
        """HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        try:
            soup = BeautifulSoup(content, 'html.parser')
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ»ã‚¹ã‚¿ã‚¤ãƒ«å‰Šé™¤
            for script in soup(["script", "style"]):
                script.decompose()
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            text = soup.get_text()
            # æ”¹è¡Œãƒ»ç©ºç™½ã®æ­£è¦åŒ–
            text = re.sub(r'\s+', ' ', text).strip()
            return text
        except Exception as e:
            logger.error(f"HTMLå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_pdf(file_path: str) -> str:
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        text = ""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
        except Exception as e:
            logger.error(f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return text
    
    @staticmethod
    def extract_text_from_docx(file_path: str) -> str:
        """DOCXã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        try:
            doc = Document(file_path)
            return '\n'.join([paragraph.text for paragraph in doc.paragraphs if paragraph.text])
        except Exception as e:
            logger.error(f"DOCXå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_pptx(file_path: str) -> str:
        """PPTXã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        try:
            prs = Presentation(file_path)
            text = ""
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text:
                        text += shape.text + "\n"
            return text
        except Exception as e:
            logger.error(f"PPTXå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

class SudachiAnalyzer:
    """Sudachiå½¢æ…‹ç´ è§£æï¼ˆSudachiDict-fullä½¿ç”¨ï¼‰"""
    
    def __init__(self):
        # SudachiPyã§Fullè¾æ›¸ã‚’ä½¿ç”¨ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«çµŒç”±ï¼‰
        from sudachipy import tokenizer, dictionary
        
        try:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            self.tokenizer_obj = dictionary.Dictionary().create()
            self.mode = tokenizer.Tokenizer.SplitMode.A
            logger.info("âœ… Sudachiè¾æ›¸ã‚’ä½¿ç”¨ã—ã¦åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"Sudachiè¾æ›¸ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            raise
    
    def analyze(self, text: str) -> List[Dict]:
        """ãƒ†ã‚­ã‚¹ãƒˆè§£æ"""
        if not text:
            return []
            
        words = []
        try:
            tokens = self.tokenizer_obj.tokenize(text, self.mode)
            
            for token in tokens:
                # å°‚é–€ç”¨èªã‚‰ã—ã„ã‚‚ã®ï¼ˆåè©ã€è¤‡åˆèªãªã©ï¼‰ã‚’æŠ½å‡º
                pos = token.part_of_speech()[0]
                surface = token.surface()
                
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶
                if (pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©'] and 
                    len(surface) >= 2 and 
                    not surface.isdigit() and
                    surface not in ['ã“ã¨', 'ã‚‚ã®', 'ãŸã‚']):
                    
                    words.append({
                        'word': surface,
                        'reading': token.reading_form() or surface,
                        'part_of_speech': pos,
                    })
        except Exception as e:
            logger.error(f"å½¢æ…‹ç´ è§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return words

class NewWordDetector:
    """æ–°èªæ¤œå‡ºï¼ˆllama-cliä½¿ç”¨ï¼‰"""
    
    def __init__(self, model_path: str, cli_path: str = "llama-cli"):
        self.model_path = model_path
        self.cli_path = cli_path
        
        # llama-cliã®å‹•ä½œç¢ºèª
        try:
            result = subprocess.run([self.cli_path, "--help"], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode != 0:
                raise RuntimeError(f"llama-cli not found or not working: {self.cli_path}")
            logger.info("âœ… llama-cli is ready")
        except Exception as e:
            logger.error(f"llama-cli initialization failed: {e}")
            raise
    
    def is_new_word(self, word: str, context: str = "") -> Tuple[bool, float, str]:
        """æ–°èªã‹ã©ã†ã‹åˆ¤å®š"""
        prompt = f"""ä»¥ä¸‹ã®å˜èªãŒåŒ»ç™‚ãƒ»åšç”ŸåŠ´åƒé–¢é€£ã®æ–°ã—ã„å°‚é–€ç”¨èªã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

å˜èª: {word}
æ–‡è„ˆ: {context[:200]}

åˆ¤å®šåŸºæº–:
- æ—¢å­˜ã®ä¸€èˆ¬çš„ãªå˜èªã§ã¯ãªã„
- å°‚é–€çš„ãªæ¦‚å¿µã‚’è¡¨ã—ã¦ã„ã‚‹  
- æ¯”è¼ƒçš„æ–°ã—ã„ç”¨èªã§ã‚ã‚‹å¯èƒ½æ€§

å›ç­”ã¯ä»¥ä¸‹ã®å½¢å¼ã§ç­”ãˆã¦ãã ã•ã„:
åˆ¤å®š: [æ–°èª/æ—¢å­˜èª]
ä¿¡é ¼åº¦: [0.0-1.0]
ç†ç”±: [åˆ¤å®šç†ç”±ã‚’ç°¡æ½”ã«]"""

        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¸¡ã™
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                f.write(prompt)
                prompt_file = f.name
            
            # llama-cliå®Ÿè¡Œ
            cmd = [
                self.cli_path,
                "-m", self.model_path,
                "-f", prompt_file,
                "-n", "200",            # max tokens
                "--temp", "0.1",        # temperature
                "--top-k", "40",        # top-k sampling
                "--top-p", "0.9",       # top-p sampling
                "-c", "2048",           # context size
                "--threads", "4"        # threads
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            os.unlink(prompt_file)
            
            if result.returncode != 0:
                logger.error(f"llama-cli error: {result.stderr}")
                return False, 0.0, "LLMå®Ÿè¡Œã‚¨ãƒ©ãƒ¼"
            
            response = result.stdout.strip()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æï¼ˆç°¡ç•¥ç‰ˆï¼‰
            is_new = "æ–°èª" in response
            
            # ä¿¡é ¼åº¦æŠ½å‡ºï¼ˆæ­£è¦è¡¨ç¾ã§ï¼‰
            confidence_match = re.search(r'ä¿¡é ¼åº¦[:ï¼š]\s*([0-9.]+)', response)
            confidence = float(confidence_match.group(1)) if confidence_match else (0.8 if is_new else 0.2)
            
            # ç†ç”±æŠ½å‡º
            reason_match = re.search(r'ç†ç”±[:ï¼š]\s*(.+)', response, re.MULTILINE | re.DOTALL)
            reasoning = reason_match.group(1).strip() if reason_match else response
            
            return is_new, confidence, reasoning
            
        except subprocess.TimeoutExpired:
            logger.error(f"llama-cli timeout for word: {word}")
            return False, 0.0, "LLMå®Ÿè¡Œã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"
        except Exception as e:
            logger.error(f"llama-cli execution error: {e}")
            return False, 0.0, f"LLMå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}"

class MhlwCrawler:
    """ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.db = SupabaseClient()
        self.processor = DocumentProcessor()
        self.analyzer = SudachiAnalyzer()
        
        # LLMè¨­å®š
        model_path = os.environ.get('LLAMA_MODEL_PATH', 'models/ggml-model-Q4_K_M.gguf')
        cli_path = os.environ.get('LLAMA_CLI_PATH', 'llama-cli')
        self.detector = NewWordDetector(model_path, cli_path)
        
        self.base_url = "https://www.mhlw.go.jp"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; MHLW Terminology Research Bot; +https://github.com/)'
        })
        
        logger.info("ğŸš€ MhlwCrawleråˆæœŸåŒ–å®Œäº†")
    
    def get_urls_to_crawl(self) -> List[str]:
        """ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å¯¾è±¡URLå–å¾—"""
        urls = []
        
        # åšåŠ´çœã®ä¸»è¦ãƒšãƒ¼ã‚¸ã‹ã‚‰é–‹å§‹
        start_urls = [
            f"{self.base_url}/stf/seisakunitsuite/bunya/kenkou_iryou/",
            f"{self.base_url}/stf/seisakunitsuite/bunya/koyou_roudou/",
            f"{self.base_url}/toukei/",
            f"{self.base_url}/shingi/",
        ]
        
        logger.info(f"ğŸ“¡ {len(start_urls)}å€‹ã®ã‚¹ã‚¿ãƒ¼ãƒˆURLã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹")
        
        for start_url in start_urls:
            try:
                logger.info(f"ğŸ” {start_url} ã‚’è§£æä¸­...")
                response = self.session.get(start_url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # PDF, DOCX, PPTXãƒªãƒ³ã‚¯åé›†
                file_links = 0
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if any(ext in href.lower() for ext in ['.pdf', '.docx', '.pptx']):
                        full_url = urljoin(start_url, href)
                        if full_url not in urls:
                            urls.append(full_url)
                            file_links += 1
                
                # HTMLãƒšãƒ¼ã‚¸ã‚‚è¿½åŠ ï¼ˆåŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ï¼‰
                html_links = 0
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if (href.startswith('/') or 'mhlw.go.jp' in href) and not any(ext in href.lower() for ext in ['.pdf', '.docx', '.pptx']):
                        full_url = urljoin(start_url, href)
                        if full_url not in urls and len(urls) < 100:  # ä¸Šé™è¨­å®š
                            urls.append(full_url)
                            html_links += 1
                
                logger.info(f"âœ… {start_url}: ãƒ•ã‚¡ã‚¤ãƒ«{file_links}ä»¶ã€HTML{html_links}ä»¶ã‚’ç™ºè¦‹")
                time.sleep(1)  # é–“éš”ã‚’ç©ºã‘ã‚‹
                
            except Exception as e:
                logger.error(f"âŒ URLåé›†ã‚¨ãƒ©ãƒ¼ {start_url}: {e}")
        
        # é‡è¤‡é™¤å»
        unique_urls = list(set(urls))
        logger.info(f"ğŸ¯ åˆè¨ˆ {len(unique_urls)} å€‹ã®URLã‚’åé›†å®Œäº†")
        
        return unique_urls[:50]  # æœ€åˆã®50å€‹ã«åˆ¶é™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    
    def process_url(self, url: str) -> Optional[Dict]:
        """å˜ä¸€URLå‡¦ç†"""
        try:
            logger.info(f"ğŸ”„ å‡¦ç†é–‹å§‹: {url}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            content_hash = hashlib.md5(response.content).hexdigest()
            
            # æ—¢å‡¦ç†ãƒã‚§ãƒƒã‚¯
            if self.db.is_url_processed(url, content_hash):
                logger.info(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å‡¦ç†ï¼‰: {url}")
                return None
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¤å®š
            content_type = self._get_content_type(url, response.headers.get('content-type', ''))
            
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            text = self._extract_text(response.content, content_type, url)
            if not text or len(text) < 50:
                logger.warning(f"âš ï¸  ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—ã¾ãŸã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¸è¶³: {url}")
                return None
            
            logger.info(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {len(text)}æ–‡å­—")
            
            # å½¢æ…‹ç´ è§£æ
            words = self.analyzer.analyze(text)
            logger.info(f"ğŸ”¤ å½¢æ…‹ç´ è§£æå®Œäº†: {len(words)}èªã‚’æŠ½å‡º")
            
            # æ—¢å­˜è¾æ›¸ã¨ç…§åˆ
            dictionary_words = self.db.get_dictionary_words()
            new_candidates = []
            
            for word_data in words:
                word = word_data['word']
                if word not in dictionary_words and len(word) >= 2:
                    # LLMåˆ¤å®šï¼ˆã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼‰
                    if len(new_candidates) < 5:  # æœ€åˆã®5èªã®ã¿LLMåˆ¤å®š
                        is_new, confidence, reasoning = self.detector.is_new_word(word, text[:500])
                        if is_new and confidence > 0.5:
                            new_candidates.append({
                                'word': word,
                                'reading': word_data['reading'],
                                'part_of_speech': word_data['part_of_speech'],
                                'confidence_score': confidence,
                                'llm_reasoning': reasoning[:200],  # ç†ç”±ã¯200æ–‡å­—ä»¥å†…
                                'source_urls': [url],
                                'frequency_count': 1
                            })
            
            # DBä¿å­˜
            url_id = self.db.save_processed_url(url, content_type, content_hash)
            if url_id and words:
                # èªæ•°åˆ¶é™
                words_to_save = words[:100] if len(words) > 100 else words
                self.db.save_extracted_words(words_to_save, url_id)
            
            for candidate in new_candidates:
                self.db.save_new_word_candidate(candidate)
            
            logger.info(f"âœ… å®Œäº†: {url} - æ–°èªå€™è£œ: {len(new_candidates)}ä»¶")
            return {
                'url': url,
                'words_count': len(words),
                'new_words_count': len(new_candidates)
            }
            
        except Exception as e:
            logger.error(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼ {url}: {e}")
            return None
    
    def _get_content_type(self, url: str, content_type_header: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—åˆ¤å®š"""
        url_lower = url.lower()
        if '.pdf' in url_lower:
            return 'pdf'
        elif '.docx' in url_lower:
            return 'docx'
        elif '.pptx' in url_lower:
            return 'pptx'
        else:
            return 'html'
    
    def _extract_text(self, content: bytes, content_type: str, url: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        if content_type == 'html':
            return self.processor.extract_text_from_html(content.decode('utf-8', errors='ignore'))
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã—ã¦å‡¦ç†
            temp_path = f"/tmp/{uuid.uuid4()}.{content_type}"
            try:
                with open(temp_path, 'wb') as f:
                    f.write(content)
                
                if content_type == 'pdf':
                    return self.processor.extract_text_from_pdf(temp_path)
                elif content_type == 'docx':
                    return self.processor.extract_text_from_docx(temp_path)
                elif content_type == 'pptx':
                    return self.processor.extract_text_from_pptx(temp_path)
            finally:
                Path(temp_path).unlink(missing_ok=True)
        
        return ""
    
    def run(self, max_workers: int = 5):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        logger.info("ğŸš€ åšåŠ´çœã‚µã‚¤ãƒˆè§£æé–‹å§‹")
        start_time = time.time()
        
        # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å¯¾è±¡URLå–å¾—
        urls = self.get_urls_to_crawl()
        if not urls:
            logger.error("âŒ ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å¯¾è±¡URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        logger.info(f"ğŸ¯ å¯¾è±¡URLæ•°: {len(urls)}")
        
        total_processed = 0
        total_new_words = 0
        
        # ä¸¦åˆ—å‡¦ç†
        logger.info(f"ğŸ‘¥ ä¸¦åˆ—å‡¦ç†é–‹å§‹ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}ï¼‰")
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self.process_url, url): url for url in urls}
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    if result:
                        total_processed += 1
                        total_new_words += result['new_words_count']
                        logger.info(f"ğŸ“Š é€²æ—: {total_processed}/{len(urls)} å®Œäº†")
                except Exception as e:
                    logger.error(f"âŒ {url} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        
        elapsed_time = time.time() - start_time
        logger.info(f"ğŸ‰ å‡¦ç†å®Œäº†: {total_processed}URLå‡¦ç†, {total_new_words}æ–°èªå€™è£œç™ºè¦‹, {elapsed_time:.1f}ç§’")

if __name__ == "__main__":
    # ç›´æ¥å®Ÿè¡Œç”¨
    import argparse
    
    parser = argparse.ArgumentParser(description='åšåŠ´çœã‚µã‚¤ãƒˆå°‚é–€ç”¨èªè§£æ')
    parser.add_argument('--workers', type=int, default=3, help='ä¸¦åˆ—å‡¦ç†æ•°')
    args = parser.parse_args()
    
    crawler = MhlwCrawler()
    crawler.run(max_workers=args.workers)
